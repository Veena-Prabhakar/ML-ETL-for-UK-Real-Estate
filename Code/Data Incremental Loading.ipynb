{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebba8feb-d2de-4b24-afcd-8efc9bc43738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:13.391161Z",
     "iopub.status.busy": "2024-09-12T12:39:13.391161Z",
     "iopub.status.idle": "2024-09-12T12:39:19.426388Z",
     "shell.execute_reply": "2024-09-12T12:39:19.425362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in c:\\users\\mailv\\anaconda3\\lib\\site-packages (2.0.25)\n",
      "Requirement already satisfied: psycopg2 in c:\\users\\mailv\\anaconda3\\lib\\site-packages (2.9.9)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\mailv\\anaconda3\\lib\\site-packages (from sqlalchemy) (4.9.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\mailv\\anaconda3\\lib\\site-packages (from sqlalchemy) (3.0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in c:\\users\\mailv\\anaconda3\\lib\\site-packages (2.9.9)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages for SQLAlchemy and PostgreSQL support\n",
    "! pip install sqlalchemy psycopg2\n",
    "\n",
    "! pip install psycopg2-binary\n",
    "\n",
    "import numpy as np  # Importing NumPy for numerical operations\n",
    "import pandas as pd  # Importing Pandas for data manipulation and analysis\n",
    "\n",
    "from datetime import datetime, timedelta  # Importing datetime and timedelta for handling dates and time deltas\n",
    "\n",
    "from sqlalchemy import create_engine  # Importing Create_engine for connecting to SQL databases\n",
    "import joblib  # Importing Joblib for saving and loading Python objects efficiently\n",
    "import psycopg2  # Importing psycopg2 provides PostgreSQL database adapter for Python\n",
    "from contextlib import contextmanager  # Importing contextmanager for creating and managing a context manager\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin  # Importing Base classes for custom Transformers\n",
    "from sklearn.compose import ColumnTransformer  # Importing ColumnTransformer for applying transformers to specific columns\n",
    "from sklearn.ensemble import RandomForestRegressor  # Importing RandomForest for regression tasks\n",
    "from sklearn.impute import SimpleImputer  # Importing SimpleImputer for handling missing values\n",
    "from sklearn.model_selection import train_test_split  # Importing train_test_split to split data into train and test sets\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion  # Importing Pipeline and FeatureUnion for creating machine learning workflows\n",
    "from sklearn.preprocessing import (  \n",
    "    RobustScaler,  # Importing RobustScaler for scaling features robust to outliers\n",
    "    StandardScaler,  # Importing StandardScaler for standardizing features\n",
    "    OrdinalEncoder,  # Importing OrdinalEncoder for encoding categorical features\n",
    "    FunctionTransformer  # Importing FunctionTransformer for creating custom data transformations\n",
    ")\n",
    "\n",
    "import logging  # Importing  logging module for event tracking and debugging\n",
    "import os  # Importing OS module for interacting with the operating system\n",
    "import warnings  # Warnings module to control warning messages\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys  # Importing  Sys module to interact with the Python runtime environment\n",
    "sys.path.append('../Scripts/')  # Adding the directory containing .py modules to the system path\n",
    "\n",
    "# Specific imports from custom scripts\n",
    "from data_transformations_1 import main_processing_pipeline  # Importing the specific function\n",
    "\n",
    "from data_transformations_2 import (  # Importing custom transformers and utilities\n",
    "    group_mean_imputer1,\n",
    "    group_mean_imputer2,\n",
    "    feature_creation,\n",
    "    cap_floor_transformer,\n",
    "    encoding_transformer,\n",
    "    robust_scaler_price,\n",
    "    standard_scaler,\n",
    "    scaling_normalizing_transformer,\n",
    "    desired_column_order,\n",
    "    GroupMeanImputer,\n",
    "    FeatureCreator,\n",
    "    EncodingWithNames,\n",
    "    ColumnTransformerDf,\n",
    "    ColumnOrderTransformer,\n",
    "    drop_columns1,\n",
    "    cap_floor,\n",
    "    cap_floor_func,\n",
    "    drop_columns2,\n",
    "    feature_manipulation,\n",
    "    handle_infinite_values\n",
    ")\n",
    "\n",
    "from data_transformations_3 import apply_imputations, prepare_data  # Importing functions for data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdb16307-3951-4113-97fc-4f9f70c8a2ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:19.429074Z",
     "iopub.status.busy": "2024-09-12T12:39:19.429074Z",
     "iopub.status.idle": "2024-09-12T12:39:19.433649Z",
     "shell.execute_reply": "2024-09-12T12:39:19.433649Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "notebook_name = 'Data Incremental Loading' \n",
    "\n",
    "# Paths for the log directories\n",
    "info_log_path = f'../Logs/info/{notebook_name}_info.log'\n",
    "\n",
    "# Creating directories if they don't exist\n",
    "os.makedirs(os.path.dirname(info_log_path), exist_ok=True)\n",
    "\n",
    "# Clearing any previous handlers if re-running this setup\n",
    "logger = logging.getLogger()\n",
    "while logger.handlers:\n",
    "    logger.handlers.pop()\n",
    "\n",
    "# Configuring logging\n",
    "info_logger = logging.getLogger('info_logger')\n",
    "\n",
    "info_handler = logging.FileHandler(info_log_path, mode='a')  # Append mode\n",
    "\n",
    "info_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Consistent formatter for both handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "info_handler.setFormatter(formatter)\n",
    "\n",
    "# Adding handlers to the loggers\n",
    "info_logger.addHandler(info_handler)\n",
    "\n",
    "info_logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dc7503-a438-4592-b6c7-c9cffc051f4d",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74f3ab28-9766-46de-829b-11d9a316c84d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:19.436683Z",
     "iopub.status.busy": "2024-09-12T12:39:19.436683Z",
     "iopub.status.idle": "2024-09-12T12:39:26.344206Z",
     "shell.execute_reply": "2024-09-12T12:39:26.342305Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reading the Data_Extraction_combined_df Excel file into a DataFrame\n",
    "Data_Extraction_combined_df = pd.read_excel('../Data/Output/Data_Extraction_combined_df.xlsx')\n",
    "\n",
    "# Creating a copy of the Data_Extraction_combined_df DataFrame for further ETL processing\n",
    "ETL_extraction_load=Data_Extraction_combined_df.copy()\n",
    "\n",
    "# Selecting and displaying the 'Transfer Month-Year' and 'Date' columns from the ETL_extraction_load DataFrame\n",
    "ETL_extraction_load[['Transfer Month-Year', 'Date']]\n",
    "    \n",
    "info_logger.info(\"Read the source data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3fdb33-72df-4fc2-81c4-de7d708180cb",
   "metadata": {},
   "source": [
    "### Sample increment Data for Jan 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "089d54f3-6a8f-4c52-9820-8dd7c88d3c79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:26.347490Z",
     "iopub.status.busy": "2024-09-12T12:39:26.347490Z",
     "iopub.status.idle": "2024-09-12T12:39:26.566998Z",
     "shell.execute_reply": "2024-09-12T12:39:26.566077Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Defining a function to sample an equal fraction of rows within each group\n",
    "def sample_equal_within_group(group, frac=0.1):\n",
    "    return group.sample(frac=frac, random_state=42)\n",
    "\n",
    "# Applying the sampling function to each group defined by 'Local authority code' and reset the index\n",
    "grouped_sampled_df = ETL_extraction_load.groupby(['Local authority code ']).apply(sample_equal_within_group).reset_index(drop=True)\n",
    "\n",
    "# Updating the 'Transfer Month-Year' and 'Date' columns to reflect a new date\n",
    "grouped_sampled_df['Transfer Month-Year'] = 'Jan-2024'\n",
    "grouped_sampled_df['Date'] = 'Jan-2024'\n",
    "\n",
    "# Adding the current timestamp to the 'extraction_timestamp' column\n",
    "grouped_sampled_df['extraction_timestamp'] = datetime.now() \n",
    "\n",
    "# Creating a copy of the sampled data for further testing or incremental data processing\n",
    "new_incremental_data = grouped_sampled_df.copy()\n",
    "    \n",
    "info_logger.info(\"Sample data generation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff9259-e107-4698-8481-c51b40f3cc85",
   "metadata": {},
   "source": [
    "#### Metadata timestamp check for filtering requried data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0cbee88-6d1f-48e4-8868-d4da44d780f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:26.572443Z",
     "iopub.status.busy": "2024-09-12T12:39:26.571057Z",
     "iopub.status.idle": "2024-09-12T12:39:26.622989Z",
     "shell.execute_reply": "2024-09-12T12:39:26.622989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 348 new records after filtering by the minimum last extraction date: 2024-09-12 13:39:05.010491.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Establishing a connection to the PostgreSQL database\n",
    "connection = psycopg2.connect(\n",
    "    dbname='UK Real Estate DB',\n",
    "    user='postgres',\n",
    "    password='123!@*qweQWE',\n",
    "    host='localhost',\n",
    "    port='5432'\n",
    ")\n",
    "\n",
    "# Listing target tables in the database to check for the last extracted date\n",
    "target_tables = ['region_dimension', 'date_dimension', 'sales_transactions_fact', \n",
    "                 'rental_dimension', 'vehicle_dimension', 'property_type_dimension', \n",
    "                 'demographics_dimension', 'education_employment_dimension']\n",
    "\n",
    "# Initialising a variable to store the minimum last extracted date across all target tables\n",
    "min_last_extracted_date = None\n",
    "\n",
    "# Retrieving the minimum last extracted date across all specified target tables\n",
    "with connection.cursor() as cursor:\n",
    "    query_metadata = f\"\"\"\n",
    "    SELECT MIN(last_extracted_date) as min_last_extracted_date \n",
    "    FROM metadata \n",
    "    WHERE table_name IN ({','.join([f\"'{table}'\" for table in target_tables])});\n",
    "    \"\"\"\n",
    "    cursor.execute(query_metadata)\n",
    "    result = cursor.fetchone()\n",
    "    min_last_extracted_date = result[0] if result else None\n",
    "\n",
    "# Closing the database connection after retrieving the date\n",
    "connection.close()\n",
    "\n",
    "# Filtering the incremental data based on the retrieved minimum last extraction date\n",
    "if min_last_extracted_date is not None:\n",
    "    # Filtering to get only new data since the minimum last extraction date across all target tables\n",
    "    filtered_data_for_processing = new_incremental_data[new_incremental_data['extraction_timestamp'] > min_last_extracted_date]\n",
    "    record_count = filtered_data_for_processing.shape[0]\n",
    "    \n",
    "    if record_count > 0:\n",
    "        print(f\"Processing {record_count} new records after filtering by the minimum last extraction date: {min_last_extracted_date}.\")\n",
    "    else:\n",
    "        print(\"No new data to process after filtering.\")\n",
    "else:\n",
    "    print(\"No previous extraction date found for any target table. Processing all available data.\")\n",
    "    filtered_data_for_transformation = new_incremental_data\n",
    "    record_count = filtered_data_for_processing.shape[0]\n",
    "    \n",
    "    if record_count > 0:\n",
    "        print(f\"Processing {record_count} records.\")\n",
    "    else:\n",
    "        print(\"No data available for processing.\")\n",
    "\n",
    "# The DataFrame `filtered_data_for_processing` now contains records with an extraction_timestamp greater than min_last_extracted_date\n",
    "    \n",
    "info_logger.info(\"Required data identified and filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98753080-dc0d-4e6c-b603-f7cae74d3333",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:26.625995Z",
     "iopub.status.busy": "2024-09-12T12:39:26.625995Z",
     "iopub.status.idle": "2024-09-12T12:39:26.628788Z",
     "shell.execute_reply": "2024-09-12T12:39:26.628788Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Creating a copy of the filtered data for further transformations and store it in `final_extracted_df`\n",
    "final_extracted_df = filtered_data_for_processing.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c16987c5-6331-4cab-9eeb-4ed99ba0efd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:26.632813Z",
     "iopub.status.busy": "2024-09-12T12:39:26.631804Z",
     "iopub.status.idle": "2024-09-12T12:39:27.128641Z",
     "shell.execute_reply": "2024-09-12T12:39:27.127407Z"
    }
   },
   "outputs": [],
   "source": [
    "final_extracted_df.to_excel(\"../Data/Output/final_extracted_df.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc519e5f-747c-40c9-a92c-b88281b06add",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd28d130-6e8b-43fe-9ef0-391963854a78",
   "metadata": {},
   "source": [
    "### Preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c399b8d-249a-4e4c-893d-c5dfa25fcb11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:27.132660Z",
     "iopub.status.busy": "2024-09-12T12:39:27.131562Z",
     "iopub.status.idle": "2024-09-12T12:39:27.972694Z",
     "shell.execute_reply": "2024-09-12T12:39:27.972694Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mDropping Unnecessary column:\u001b[0m\n",
      "Dropped 'extraction_timestamp' column.\n",
      "\n",
      "\u001b[1mDuplicate check:\u001b[0m\n",
      "No duplicate rows found in the DataFrame.\n",
      "\n",
      "\u001b[1mColumn Renaming:\u001b[0m\n",
      "Renamed columns.\n",
      "\n",
      "\u001b[1mNull values in Each record:\u001b[0m\n",
      "Column: District, Number of Null Records: 0\n",
      "Column: Transfer Month-Year, Number of Null Records: 0\n",
      "Column: Town/City, Number of Null Records: 55\n",
      "Column: County, Number of Null Records: 55\n",
      "Column: Price, Number of Null Records: 55\n",
      "Column: Property Type, Number of Null Records: 55\n",
      "Column: Old/New, Number of Null Records: 55\n",
      "Column: Duration, Number of Null Records: 55\n",
      "Column: PPD Category Type, Number of Null Records: 55\n",
      "Column: Record Status, Number of Null Records: 55\n",
      "Column: Region code, Number of Null Records: 0\n",
      "Column: Region name, Number of Null Records: 0\n",
      "Column: Local authority code, Number of Null Records: 0\n",
      "Column: Local authority name, Number of Null Records: 0\n",
      "Column: Date, Number of Null Records: 0\n",
      "Column: RegionName, Number of Null Records: 26\n",
      "Column: AreaCode, Number of Null Records: 26\n",
      "Column: AveragePrice, Number of Null Records: 26\n",
      "Column: Index, Number of Null Records: 26\n",
      "Column: 1m%Change, Number of Null Records: 26\n",
      "Column: SalesVolume, Number of Null Records: 26\n",
      "Column: DetachedPrice, Number of Null Records: 27\n",
      "Column: SemiDetachedPrice, Number of Null Records: 27\n",
      "Column: TerracedPrice, Number of Null Records: 27\n",
      "Column: FlatPrice, Number of Null Records: 26\n",
      "Column: CashPrice, Number of Null Records: 26\n",
      "Column: MortgagePrice, Number of Null Records: 26\n",
      "Column: MortgageIndex, Number of Null Records: 26\n",
      "Column: FTBPrice, Number of Null Records: 26\n",
      "Column: FOOPrice, Number of Null Records: 26\n",
      "Column: NewPrice, Number of Null Records: 29\n",
      "Column: NewSalesVolume, Number of Null Records: 253\n",
      "Column: OldPrice, Number of Null Records: 26\n",
      "Column: OldSalesVolume, Number of Null Records: 26\n",
      "Column: Annual change (%), Number of Null Records: 54\n",
      "Column: Rental price (£), Number of Null Records: 54\n",
      "Column: One Bedroom Rent, Number of Null Records: 54\n",
      "Column: Two Bedrooms Rent, Number of Null Records: 54\n",
      "Column: Three Bedrooms Rent, Number of Null Records: 54\n",
      "Column: Four or more Bedrooms Rent, Number of Null Records: 54\n",
      "Column: All categories Rent, Number of Null Records: 54\n",
      "Column: All ages, Number of Null Records: 25\n",
      "Column: 0-20, Number of Null Records: 25\n",
      "Column: 20-40, Number of Null Records: 25\n",
      "Column: 40-60, Number of Null Records: 25\n",
      "Column: 60+, Number of Null Records: 25\n",
      "Column: Female population, Number of Null Records: 25\n",
      "Column: Male population, Number of Null Records: 25\n",
      "Column: Area (sq km), Number of Null Records: 25\n",
      "Column: Qualification index score, Number of Null Records: 56\n",
      "Column: Qualification index rank (1 to 331), Number of Null Records: 56\n",
      "Column: No qualifications, Number of Null Records: 56\n",
      "Column: Level 1 and entry level qualifications, Number of Null Records: 56\n",
      "Column: Level 2 qualifications, Number of Null Records: 56\n",
      "Column: Apprenticeship, Number of Null Records: 56\n",
      "Column: Level 3 qualifications, Number of Null Records: 56\n",
      "Column: Level 4 qualifications and above, Number of Null Records: 56\n",
      "Column: Other qualifications, Number of Null Records: 56\n",
      "Column: Estimated number of households with at least 1 early-years or school age child, Number of Null Records: 53\n",
      "Column: Deprivation Average Score, Number of Null Records: 51\n",
      "Column: Number of those aged 16+ who are unemployed, Number of Null Records: 51\n",
      "Column: Number of those aged 16+ in employment who are employees, Number of Null Records: 51\n",
      "Column: Number of those aged 16+ in employment who are self-employed, Number of Null Records: 51\n",
      "Column: GDHI, Number of Null Records: 57\n",
      "Column: Number of Schools, Number of Null Records: 195\n",
      "Column: Headcount of Pupils(school), Number of Null Records: 195\n",
      "Column: Buses total, Number of Null Records: 52\n",
      "Column: Diesel cars total, Number of Null Records: 52\n",
      "Column: Petrol cars total, Number of Null Records: 52\n",
      "Column: HGV - Motorways, Number of Null Records: 52\n",
      "Column: HGV total, Number of Null Records: 52\n",
      "Column: Diesel LGV total, Number of Null Records: 52\n",
      "Column: Petrol LGV total, Number of Null Records: 52\n",
      "Column: LPG LGV total, Number of Null Records: 52\n",
      "Column: Personal transport (buses, cars and motorcycles), Number of Null Records: 52\n",
      "Column: Freight transport (HGV and LGV), Number of Null Records: 52\n",
      "Column: Fuel consumption by all vehicles, Number of Null Records: 52\n",
      "Total Number of Null Records in DataFrame: 3525\n",
      "\n",
      "\u001b[1mDrop records with Null values on Key attributes:\u001b[0m\n",
      "Number of records before cleaning: 348\n",
      "Number of records after cleaning: 292\n",
      "\n",
      "\u001b[1mData export:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to '../Data/Output/original_cleaned_df.xlsx'\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# calling saved preprocessing pipeline on sampled extracted df\n",
    "original_cleaned_df = main_processing_pipeline(\"../Data/Output/final_extracted_df.xlsx\")\n",
    "    \n",
    "info_logger.info(\"Applied preprocessing pipeline stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89ddcd2b-0c9e-46d8-95f8-adaa23ba9212",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:27.976218Z",
     "iopub.status.busy": "2024-09-12T12:39:27.976218Z",
     "iopub.status.idle": "2024-09-12T12:39:28.013314Z",
     "shell.execute_reply": "2024-09-12T12:39:28.012232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>District</th>\n",
       "      <th>Transfer Month-Year</th>\n",
       "      <th>Town/City</th>\n",
       "      <th>County</th>\n",
       "      <th>Price</th>\n",
       "      <th>Property Type</th>\n",
       "      <th>Old/New</th>\n",
       "      <th>Duration</th>\n",
       "      <th>PPD Category Type</th>\n",
       "      <th>Record Status</th>\n",
       "      <th>...</th>\n",
       "      <th>Diesel cars total</th>\n",
       "      <th>Petrol cars total</th>\n",
       "      <th>HGV - Motorways</th>\n",
       "      <th>HGV total</th>\n",
       "      <th>Diesel LGV total</th>\n",
       "      <th>Petrol LGV total</th>\n",
       "      <th>LPG LGV total</th>\n",
       "      <th>Personal transport (buses, cars and motorcycles)</th>\n",
       "      <th>Freight transport (HGV and LGV)</th>\n",
       "      <th>Fuel consumption by all vehicles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HARTLEPOOL</td>\n",
       "      <td>Jan-2024</td>\n",
       "      <td>BILLINGHAM</td>\n",
       "      <td>HARTLEPOOL</td>\n",
       "      <td>8.218558e+06</td>\n",
       "      <td>D</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>12.728113</td>\n",
       "      <td>17.376559</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.945570</td>\n",
       "      <td>7.966492</td>\n",
       "      <td>0.259175</td>\n",
       "      <td>1.321797e-04</td>\n",
       "      <td>31.807181</td>\n",
       "      <td>14.171370</td>\n",
       "      <td>45.978550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MIDDLESBROUGH</td>\n",
       "      <td>Jan-2024</td>\n",
       "      <td>MIDDLESBROUGH</td>\n",
       "      <td>MIDDLESBROUGH</td>\n",
       "      <td>2.853917e+07</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>27.592611</td>\n",
       "      <td>40.284392</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.553060</td>\n",
       "      <td>16.150149</td>\n",
       "      <td>0.554463</td>\n",
       "      <td>2.714230e-04</td>\n",
       "      <td>71.986032</td>\n",
       "      <td>26.257943</td>\n",
       "      <td>98.243975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REDCAR AND CLEVELAND</td>\n",
       "      <td>Jan-2024</td>\n",
       "      <td>GUISBOROUGH</td>\n",
       "      <td>REDCAR AND CLEVELAND</td>\n",
       "      <td>6.770916e+06</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>18.833614</td>\n",
       "      <td>24.777259</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.076368</td>\n",
       "      <td>10.949645</td>\n",
       "      <td>0.357697</td>\n",
       "      <td>1.841292e-04</td>\n",
       "      <td>46.393679</td>\n",
       "      <td>17.383894</td>\n",
       "      <td>63.777573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STOCKTON-ON-TEES</td>\n",
       "      <td>Jan-2024</td>\n",
       "      <td>BILLINGHAM</td>\n",
       "      <td>STOCKTON-ON-TEES</td>\n",
       "      <td>1.022323e+07</td>\n",
       "      <td>D</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>28.473491</td>\n",
       "      <td>38.617891</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>15.601339</td>\n",
       "      <td>17.406072</td>\n",
       "      <td>0.557174</td>\n",
       "      <td>2.874927e-04</td>\n",
       "      <td>70.363456</td>\n",
       "      <td>33.564873</td>\n",
       "      <td>103.928329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DARLINGTON</td>\n",
       "      <td>Jan-2024</td>\n",
       "      <td>DARLINGTON</td>\n",
       "      <td>DARLINGTON</td>\n",
       "      <td>1.498214e+07</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>16.422963</td>\n",
       "      <td>21.811948</td>\n",
       "      <td>5.67299</td>\n",
       "      <td>10.391988</td>\n",
       "      <td>10.159270</td>\n",
       "      <td>0.327314</td>\n",
       "      <td>1.695507e-04</td>\n",
       "      <td>40.606431</td>\n",
       "      <td>20.878742</td>\n",
       "      <td>61.485173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>SUTTON</td>\n",
       "      <td>Jan-2024</td>\n",
       "      <td>BANSTEAD</td>\n",
       "      <td>GREATER LONDON</td>\n",
       "      <td>1.106853e+07</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>16.226703</td>\n",
       "      <td>27.953133</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.649249</td>\n",
       "      <td>10.255186</td>\n",
       "      <td>0.126071</td>\n",
       "      <td>6.002093e-10</td>\n",
       "      <td>47.318465</td>\n",
       "      <td>13.030506</td>\n",
       "      <td>60.348972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>TOWER HAMLETS</td>\n",
       "      <td>Jan-2024</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>GREATER LONDON</td>\n",
       "      <td>1.081940e+08</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>16.663450</td>\n",
       "      <td>27.981879</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8.356507</td>\n",
       "      <td>15.295616</td>\n",
       "      <td>0.260907</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>49.841541</td>\n",
       "      <td>23.913031</td>\n",
       "      <td>73.754572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>WALTHAM FOREST</td>\n",
       "      <td>Jan-2024</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>GREATER LONDON</td>\n",
       "      <td>5.870112e+07</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>14.199792</td>\n",
       "      <td>24.431921</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.094199</td>\n",
       "      <td>10.936386</td>\n",
       "      <td>0.151264</td>\n",
       "      <td>9.536322e-09</td>\n",
       "      <td>41.783746</td>\n",
       "      <td>17.181848</td>\n",
       "      <td>58.965594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>WANDSWORTH</td>\n",
       "      <td>Jan-2024</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>GREATER LONDON</td>\n",
       "      <td>2.927223e+08</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>14.453811</td>\n",
       "      <td>23.925686</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.476196</td>\n",
       "      <td>11.080560</td>\n",
       "      <td>0.149713</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>44.308423</td>\n",
       "      <td>15.706469</td>\n",
       "      <td>60.014892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>WESTMINSTER</td>\n",
       "      <td>Jan-2024</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>GREATER LONDON</td>\n",
       "      <td>4.057749e+08</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>17.826220</td>\n",
       "      <td>19.983846</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.535842</td>\n",
       "      <td>15.105449</td>\n",
       "      <td>0.259924</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>49.441613</td>\n",
       "      <td>21.901215</td>\n",
       "      <td>71.342828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>292 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 District Transfer Month-Year      Town/City  \\\n",
       "0              HARTLEPOOL            Jan-2024     BILLINGHAM   \n",
       "1           MIDDLESBROUGH            Jan-2024  MIDDLESBROUGH   \n",
       "2    REDCAR AND CLEVELAND            Jan-2024    GUISBOROUGH   \n",
       "3        STOCKTON-ON-TEES            Jan-2024     BILLINGHAM   \n",
       "4              DARLINGTON            Jan-2024     DARLINGTON   \n",
       "..                    ...                 ...            ...   \n",
       "316                SUTTON            Jan-2024       BANSTEAD   \n",
       "317         TOWER HAMLETS            Jan-2024         LONDON   \n",
       "318        WALTHAM FOREST            Jan-2024         LONDON   \n",
       "319            WANDSWORTH            Jan-2024         LONDON   \n",
       "320           WESTMINSTER            Jan-2024         LONDON   \n",
       "\n",
       "                   County         Price Property Type Old/New Duration  \\\n",
       "0              HARTLEPOOL  8.218558e+06             D       N        F   \n",
       "1           MIDDLESBROUGH  2.853917e+07             T       N        F   \n",
       "2    REDCAR AND CLEVELAND  6.770916e+06             S       N        F   \n",
       "3        STOCKTON-ON-TEES  1.022323e+07             D       N        F   \n",
       "4              DARLINGTON  1.498214e+07             T       N        F   \n",
       "..                    ...           ...           ...     ...      ...   \n",
       "316        GREATER LONDON  1.106853e+07             F       N        F   \n",
       "317        GREATER LONDON  1.081940e+08             F       N        L   \n",
       "318        GREATER LONDON  5.870112e+07             F       N        F   \n",
       "319        GREATER LONDON  2.927223e+08             F       N        L   \n",
       "320        GREATER LONDON  4.057749e+08             F       N        L   \n",
       "\n",
       "    PPD Category Type Record Status  ... Diesel cars total Petrol cars total  \\\n",
       "0                   A             A  ...         12.728113         17.376559   \n",
       "1                   A             A  ...         27.592611         40.284392   \n",
       "2                   A             A  ...         18.833614         24.777259   \n",
       "3                   A             A  ...         28.473491         38.617891   \n",
       "4                   A             A  ...         16.422963         21.811948   \n",
       "..                ...           ...  ...               ...               ...   \n",
       "316                 A             A  ...         16.226703         27.953133   \n",
       "317                 A             A  ...         16.663450         27.981879   \n",
       "318                 A             A  ...         14.199792         24.431921   \n",
       "319                 A             A  ...         14.453811         23.925686   \n",
       "320                 A             A  ...         17.826220         19.983846   \n",
       "\n",
       "    HGV - Motorways  HGV total Diesel LGV total Petrol LGV total  \\\n",
       "0           0.00000   5.945570         7.966492         0.259175   \n",
       "1           0.00000   9.553060        16.150149         0.554463   \n",
       "2           0.00000   6.076368        10.949645         0.357697   \n",
       "3           0.00000  15.601339        17.406072         0.557174   \n",
       "4           5.67299  10.391988        10.159270         0.327314   \n",
       "..              ...        ...              ...              ...   \n",
       "316         0.00000   2.649249        10.255186         0.126071   \n",
       "317         0.00000   8.356507        15.295616         0.260907   \n",
       "318         0.00000   6.094199        10.936386         0.151264   \n",
       "319         0.00000   4.476196        11.080560         0.149713   \n",
       "320         0.00000   6.535842        15.105449         0.259924   \n",
       "\n",
       "    LPG LGV total  Personal transport (buses, cars and motorcycles)  \\\n",
       "0    1.321797e-04                                         31.807181   \n",
       "1    2.714230e-04                                         71.986032   \n",
       "2    1.841292e-04                                         46.393679   \n",
       "3    2.874927e-04                                         70.363456   \n",
       "4    1.695507e-04                                         40.606431   \n",
       "..            ...                                               ...   \n",
       "316  6.002093e-10                                         47.318465   \n",
       "317  0.000000e+00                                         49.841541   \n",
       "318  9.536322e-09                                         41.783746   \n",
       "319  0.000000e+00                                         44.308423   \n",
       "320  0.000000e+00                                         49.441613   \n",
       "\n",
       "     Freight transport (HGV and LGV)  Fuel consumption by all vehicles  \n",
       "0                          14.171370                         45.978550  \n",
       "1                          26.257943                         98.243975  \n",
       "2                          17.383894                         63.777573  \n",
       "3                          33.564873                        103.928329  \n",
       "4                          20.878742                         61.485173  \n",
       "..                               ...                               ...  \n",
       "316                        13.030506                         60.348972  \n",
       "317                        23.913031                         73.754572  \n",
       "318                        17.181848                         58.965594  \n",
       "319                        15.706469                         60.014892  \n",
       "320                        21.901215                         71.342828  \n",
       "\n",
       "[292 rows x 77 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_cleaned_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4c979bd-fddc-4035-ba32-ef5b8bfec950",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:28.018245Z",
     "iopub.status.busy": "2024-09-12T12:39:28.018245Z",
     "iopub.status.idle": "2024-09-12T12:39:28.268097Z",
     "shell.execute_reply": "2024-09-12T12:39:28.266988Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ETL_Database_Df = original_cleaned_df.copy() # Creating a copy of the cleaned DataFrame to use for the ETL process.\n",
    "\n",
    "# Loading the saved first stage pipeline for preprocessing\n",
    "ETL_first_stage_pipeline = joblib.load('../Models/first_stage_pipeline.pkl')\n",
    "\n",
    "# Applying the first stage of preprocessing to the ETL database DataFrame\n",
    "ETL_first_stage_output = ETL_first_stage_pipeline.transform(ETL_Database_Df)\n",
    "\n",
    "# Loading the saved second stage pipeline for further preprocessing\n",
    "ETL_second_stage_pipeline = joblib.load('../Models/second_stage_pipeline.pkl')\n",
    "\n",
    "# Applying the second stage of preprocessing to the output from the first stage\n",
    "ETL_second_stage_output = ETL_second_stage_pipeline.transform(ETL_first_stage_output)\n",
    "   \n",
    "info_logger.info(\"Applied full stage pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5e901e6-dde9-4906-ae17-65ab3f834c2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:28.272173Z",
     "iopub.status.busy": "2024-09-12T12:39:28.272173Z",
     "iopub.status.idle": "2024-09-12T12:39:31.516516Z",
     "shell.execute_reply": "2024-09-12T12:39:31.514854Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Using the function from the script to perform imputations\n",
    "ETL_second_stage_output = apply_imputations(ETL_second_stage_output)\n",
    "\n",
    "# Preparing data by splitting into features and targets and performing a train-test split\n",
    "ETL_second_stage_output, ETL_X, ETL_y, X_train, X_test, y_train, y_test = prepare_data(ETL_second_stage_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f88661-93a2-4de1-aa9a-0c50d4a47161",
   "metadata": {},
   "source": [
    "### Append Predicted Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c999252c-c33d-413e-bb4b-93736494c55a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:31.520646Z",
     "iopub.status.busy": "2024-09-12T12:39:31.519631Z",
     "iopub.status.idle": "2024-09-12T12:39:31.620032Z",
     "shell.execute_reply": "2024-09-12T12:39:31.618925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Prices:\n",
      "[-1.91902154e-01  4.62258262e-01 -2.97732569e-01 -5.78272797e-02\n",
      " -3.61363657e-02 -2.86150357e-01  1.20127991e+00 -2.45971435e-01\n",
      "  1.44290673e-01  8.18628783e-01 -7.42279321e-03 -1.38141372e-01\n",
      " -4.41899806e-01  2.90982648e+00  2.12396331e+00  2.06903734e+00\n",
      " -1.53917496e-01  3.17242494e+00 -2.92411953e-01  1.55151184e+00\n",
      "  1.25940198e+00  3.57780808e-01  8.47843639e-01  2.97289485e+00\n",
      "  1.34613177e-01  3.25033036e+00  8.44634204e-01  1.78455247e+00\n",
      "  5.31607998e-01 -1.86378707e-01  9.06543592e-01  8.93117083e-02\n",
      "  3.56359451e-01  3.21471266e+00  1.22838291e+00  9.12970881e-02\n",
      "  9.39141911e-01  8.98222269e-01  3.24286955e+00  8.72378095e-01\n",
      "  2.42751044e+00 -4.27767024e-01  7.80796644e-02 -1.98721015e-02\n",
      " -1.00067548e-01 -1.06701118e-01 -4.34724707e-02  4.88563049e-02\n",
      "  7.82486097e-01 -1.45104678e-01 -1.22846847e-01  1.47835657e+00\n",
      "  4.65669644e-02 -1.09046944e-02  3.13098490e-02  2.59068634e-01\n",
      " -1.25589977e-01 -3.93325725e-01 -5.70818459e-03 -3.69380272e-03\n",
      "  3.13321844e+00 -1.45258718e-01 -3.64964527e-01 -5.86664036e-02\n",
      "  3.09763378e-01 -4.34303704e-01 -4.54802558e-01  6.23888240e-01\n",
      " -3.56924168e-01 -1.82110200e-01 -2.84181787e-01 -3.47490353e-01\n",
      " -2.72318164e-01 -2.98197966e-01  2.00105223e+00 -3.34631439e-01\n",
      " -4.33024236e-01 -3.82730141e-01 -1.72567719e-01 -4.17724245e-01\n",
      " -4.53783367e-01  1.80215284e+00  3.39073203e-01 -3.73949008e-01\n",
      " -3.69690471e-01 -3.55161668e-01  3.31096245e-01 -2.64614101e-01\n",
      "  5.51646449e-01 -1.23034245e-01  3.48158589e-01  2.75182665e+00\n",
      " -3.62767700e-01  7.26364881e-01 -4.29253324e-01 -2.65019313e-01\n",
      " -2.01833391e-01 -1.59231050e-01  2.40095732e+00 -3.23857431e-01\n",
      " -4.54173830e-01  1.00333824e+00 -2.35851996e-01 -2.15610096e-01\n",
      " -1.45537294e-01 -3.82328612e-01  8.24726640e-01  5.13192033e-01\n",
      "  5.65757559e-02 -7.08425136e-02 -1.36621018e-01 -1.62446272e-01\n",
      " -1.68931701e-03 -2.06214308e-01  1.55609137e-04 -1.32302072e-02\n",
      "  2.99951066e-01  5.91698377e-02 -3.30238105e-01 -1.31571336e-01\n",
      "  1.34134579e+00  9.56638031e-02  4.19659828e-01 -2.63637315e-01\n",
      " -1.64967944e-01  1.66250448e-01  4.24308239e-03 -2.69547715e-01\n",
      " -1.40320849e-01 -1.68841128e-01 -3.54910636e-01  3.86245372e-01\n",
      "  8.06118195e-02 -2.56973595e-01 -2.97845809e-01 -3.59033100e-01\n",
      " -3.18597300e-02 -4.47875612e-01  8.70406786e-01 -3.49484954e-01\n",
      " -4.55316668e-01 -2.07861878e-01 -3.43160897e-01 -3.87598975e-01\n",
      "  2.06938975e-01  5.21896448e-01 -1.00302232e-02 -4.01488382e-01\n",
      " -4.27881030e-01 -3.97557304e-01 -2.46245563e-01  1.17042085e-01\n",
      " -4.51447174e-01  3.53283184e-01 -2.74633419e-02 -2.23814892e-01\n",
      " -2.32411607e-01 -3.61382770e-01 -3.98542551e-01  1.51899347e+00\n",
      "  5.02666912e-01 -2.43634486e-01 -3.39324004e-01  1.31572238e+00\n",
      " -2.20323284e-01 -1.19214827e-01 -3.43493693e-01  1.14361447e+00\n",
      "  1.25887646e+00  3.85590886e-01 -2.00829595e-01  2.70222123e-01\n",
      "  2.56512965e-01  4.11727461e+00 -6.06471292e-02 -7.77557099e-02\n",
      " -2.13844761e-01 -1.30085444e-04 -2.88876254e-01 -4.12051212e-01\n",
      " -2.94466145e-01 -3.95405468e-01 -1.59358570e-01 -3.16422867e-01\n",
      "  4.01725347e-01 -3.29496017e-01  1.13422840e+00 -3.49314191e-01\n",
      "  1.71536172e-01  2.74709409e-01  5.67954305e-02 -1.66328929e-01\n",
      "  1.98441281e-02 -1.59486806e-01 -5.45651264e-02 -1.38350423e-01\n",
      " -4.27499263e-01  3.71273919e-02  7.23596484e-01 -4.61414317e-01\n",
      " -1.23890683e-01  4.98156044e-02 -3.72963538e-01  4.18422487e-02\n",
      " -7.20265845e-02  1.71796686e-01 -2.89135488e-01  1.46219542e+00\n",
      "  2.03291593e-02  1.00641347e-01  2.03060851e+00 -3.30658962e-01\n",
      " -2.61809854e-01  5.92979356e-01  9.33139321e-01 -4.03139228e-01\n",
      " -2.98941371e-01  1.24903165e+00 -2.88144095e-02 -1.91260626e-01\n",
      "  6.09695679e-01 -1.93778424e-01 -8.82661550e-02  8.66682946e-01\n",
      "  6.46142940e-02  1.66525846e+00  3.37855423e-01 -2.33781768e-01\n",
      "  1.24259064e+00  1.75120694e+00 -2.38511109e-01  1.06735792e+00\n",
      "  1.09439905e-01 -2.10544224e-01  4.29691524e+00 -2.35870988e-01\n",
      "  7.20917254e-01  3.52890327e-01  2.47339778e-01  3.05181559e-01\n",
      " -1.98747205e-01  3.91144782e+00  2.92229000e+00 -8.12971462e-02\n",
      " -4.31070590e-01 -1.26147571e-01  3.74396619e+00  3.18748805e+00\n",
      " -1.44072272e-01 -3.95159506e-01  5.24572398e-01 -2.57736318e-01\n",
      "  1.02051765e-01  7.33273208e-01 -3.49563275e-01 -1.28945577e-01\n",
      "  6.84715869e-01 -1.46489121e-01 -3.88175611e-01  2.35096697e+00\n",
      "  4.56628115e-02  3.11182074e+00 -7.34312848e-02  1.02062862e+00\n",
      "  4.82416909e-01  4.09026956e+00  4.32137875e-01  1.43449157e+00\n",
      "  1.54123198e+00  3.66676177e+00  4.31947670e+00  4.06373817e+00\n",
      "  4.16475396e+00  2.97031404e-01  8.09466488e-01  2.30255419e-01\n",
      "  2.66674068e-01  4.25177609e+00  4.30325878e+00  2.79737113e-01\n",
      "  4.35927993e+00  2.68400201e+00  9.91978966e-01  2.56695087e+00\n",
      "  8.94727898e-02  8.81395538e-01  4.37316750e+00 -6.29878982e-02\n",
      "  4.28931647e+00  2.64203750e+00  4.36641599e+00  4.13448042e+00]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ETL_output = ETL_second_stage_output.copy()\n",
    "\n",
    "# Loading the final combined features DataFrame from the earlier saved excel file\n",
    "final_combined_features_df = pd.read_excel('../Data/Output/final_combined_features.xlsx')\n",
    "\n",
    "# Extracting the list of selected feature names from the DataFrame\n",
    "ETL_selected_features = final_combined_features_df['Feature'].tolist()\n",
    "\n",
    "# Subsetting the unseen data to include only the selected features\n",
    "ETL_X = ETL_output[ETL_selected_features]  \n",
    "\n",
    "# Loading the pre-trained Random Forest model\n",
    "rf_model = joblib.load('../Models/RandomForest_house_price_prediction_model.pkl')\n",
    "\n",
    "# Predicting the target variable using the Random Forest model\n",
    "predictions = rf_model.predict(ETL_X)\n",
    "\n",
    "# Adding the predictions to the DataFrame\n",
    "ETL_second_stage_output['Predicted_Price'] = predictions\n",
    "\n",
    "# Displaying the predictions\n",
    "print(\"Predicted Prices:\")\n",
    "print(predictions)\n",
    "    \n",
    "info_logger.info(\"Applied Price prediction model to predict price on incremental data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a586ad1a-6fb4-4947-8c02-9bcfbd6a43b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:31.625043Z",
     "iopub.status.busy": "2024-09-12T12:39:31.625043Z",
     "iopub.status.idle": "2024-09-12T12:39:31.639126Z",
     "shell.execute_reply": "2024-09-12T12:39:31.637983Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the pipeline used for scaling\n",
    "pipeline = joblib.load('../Models/second_stage_pipeline.pkl')\n",
    "\n",
    "# Accessing the RobustScaler used for price scaling from the pipeline\n",
    "price_scaler = pipeline.named_steps['scale_normalize'].named_transformers_['robust_scaler_price']\n",
    "\n",
    "# Reshaping predictions to match the scaler's expected input dimensions\n",
    "predictions_scaled_reshaped = predictions.reshape(-1, 1)\n",
    "\n",
    "# Reversing the scaling transformation to obtain unscaled predictions\n",
    "predictions_unscaled = price_scaler.inverse_transform(predictions_scaled_reshaped).flatten()\n",
    "\n",
    "ETL_stage_output = ETL_first_stage_output.copy()\n",
    "\n",
    "# Adding the unscaled predictions to the DataFrame\n",
    "ETL_stage_output['Predicted_Price_Unscaled'] = predictions_unscaled\n",
    "\n",
    "# Displaying the actual and predicted (unscaled) prices\n",
    "ETL_stage_output[['Price','Predicted_Price_Unscaled']]\n",
    "    \n",
    "info_logger.info(\"Applied reverse scaling and appended the predicted price to original dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10a2b0f7-2e8c-47b3-8619-48dd2be79f3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:31.642136Z",
     "iopub.status.busy": "2024-09-12T12:39:31.642136Z",
     "iopub.status.idle": "2024-09-12T12:39:31.982470Z",
     "shell.execute_reply": "2024-09-12T12:39:31.981139Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# List of columns to fill null values\n",
    "null_columns = [\n",
    "    'AveragePrice_PctChange',\n",
    "    'DetachedPrice_PctChange',\n",
    "    'SemiDetachedPrice_PctChange',\n",
    "    'TerracedPrice_PctChange',\n",
    "    'FlatPrice_PctChange'\n",
    "]\n",
    "\n",
    "# Filling NaN values in specified columns by grouping and applying mean imputation\n",
    "ETL_stage_output[null_columns] = ETL_stage_output.groupby(['Local authority code', 'Region code'])[null_columns].transform(lambda x: x.fillna(x.mean()))\n",
    "ETL_stage_output[null_columns] = ETL_stage_output.groupby(['Region code'])[null_columns].transform(lambda x: x.fillna(x.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bdd204-ead8-40b4-8f74-1f748d3cdc46",
   "metadata": {},
   "source": [
    "### Column renaming and Feature addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b1d6373-0b62-4b72-8522-072755fbecc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:31.985478Z",
     "iopub.status.busy": "2024-09-12T12:39:31.985478Z",
     "iopub.status.idle": "2024-09-12T12:39:32.004326Z",
     "shell.execute_reply": "2024-09-12T12:39:32.003316Z"
    }
   },
   "outputs": [],
   "source": [
    "# Renaming columns in ETL_stage_output DataFrame for consistency and clarity\n",
    "ETL_stage_output = ETL_stage_output.rename(columns={\n",
    "    'District': 'district',\n",
    "    'Town/City': 'town_city',\n",
    "    'County': 'county',\n",
    "    'Price': 'price',\n",
    "    'Property Type': 'property_type',\n",
    "    'Old/New': 'old_new',\n",
    "    'Duration': 'duration',\n",
    "    'PPD Category Type': 'ppd_category_type',\n",
    "    'Record Status': 'record_status',\n",
    "    'Region code': 'region_code',\n",
    "    'Region name': 'region_name',\n",
    "    'Local authority code': 'local_authority_code',\n",
    "    'Local authority name': 'local_authority_name',\n",
    "    'Date': 'date',\n",
    "    'Month': 'month',\n",
    "    'Quarter': 'quarter',\n",
    "    'Year': 'year',\n",
    "    'Transfer Month-Year': 'transfer_month_year',\n",
    "    'Index': 'index',\n",
    "    'AveragePrice': 'average_price',\n",
    "    'All ages': 'all_ages',\n",
    "    '0-20': 'age_0_20',\n",
    "    '20-40': 'age_20_40',\n",
    "    '40-60': 'age_40_60',\n",
    "    '60+': 'age_60_plus',\n",
    "    'Female population': 'female_population',\n",
    "    'Male population': 'male_population',\n",
    "    'Area (sq km)': 'area_sq_km',\n",
    "    'Qualification index score': 'qualification_index_score',\n",
    "    'Qualification index rank (1 to 331)': 'qualification_index_rank',\n",
    "    'No qualifications': 'no_qualifications',\n",
    "    'Level 1 and entry level qualifications': 'level_1_and_entry_level_qualifications',\n",
    "    'Level 2 qualifications': 'level_2_qualifications',\n",
    "    'Apprenticeship': 'apprenticeship',\n",
    "    'Level 3 qualifications': 'level_3_qualifications',\n",
    "    'Level 4 qualifications and above': 'level_4_qualifications_and_above',\n",
    "    'Other qualifications': 'other_qualifications',\n",
    "    'Estimated number of households with at least 1 early-years or school age child': 'est_num_households_with_child',\n",
    "    'Deprivation Average Score': 'deprivation_average_score',\n",
    "    'Number of those aged 16+ who are unemployed': 'num_aged_16_plus_unemployed',\n",
    "    'Number of those aged 16+ in employment who are employees': 'num_aged_16_plus_employed',\n",
    "    'Number of those aged 16+ in employment who are self-employed': 'num_aged_16_plus_self_employed',\n",
    "    'One Bedroom Rent': 'one_bedroom_rent',\n",
    "    'Two Bedrooms Rent': 'two_bedrooms_rent',\n",
    "    'Three Bedrooms Rent': 'three_bedrooms_rent',\n",
    "    'Four or more Bedrooms Rent': 'four_or_more_bedrooms_rent',\n",
    "    'All categories Rent': 'all_categories_rent',\n",
    "    'GDHI': 'gdhi',\n",
    "    'Buses total': 'buses_total',\n",
    "    'Diesel cars total': 'diesel_cars_total',\n",
    "    'Petrol cars total': 'petrol_cars_total',\n",
    "    'HGV - Motorways': 'hgv_motorways',\n",
    "    'HGV total': 'hgv_total',\n",
    "    'Diesel LGV total': 'diesel_lgv_total',\n",
    "    'Petrol LGV total': 'petrol_lgv_total',\n",
    "    'LPG LGV total': 'lpg_lgv_total',\n",
    "    'Personal transport (buses, cars and motorcycles)': 'personal_transport',\n",
    "    'Freight transport (HGV and LGV)': 'freight_transport',\n",
    "    'Fuel consumption by all vehicles': 'fuel_consumption',\n",
    "    '1m%Change': 'one_m_percent_change',\n",
    "    'Annual change (%)': 'annual_change_percent',\n",
    "    'Rental price (£)': 'rental_price',\n",
    "    'SalesVolume': 'sales_volume',\n",
    "    'DetachedPrice': 'detached_price',\n",
    "    'SemiDetachedPrice': 'semi_detached_price',\n",
    "    'TerracedPrice': 'terraced_price',\n",
    "    'FlatPrice': 'flat_price',\n",
    "    'CashPrice': 'cash_price',\n",
    "    'MortgagePrice': 'mortgage_price',\n",
    "    'MortgageIndex': 'mortgage_index',\n",
    "    'FTBPrice': 'ftb_price',\n",
    "    'FOOPrice': 'foo_price',\n",
    "    'NewPrice': 'new_price',\n",
    "    'OldPrice': 'old_price',\n",
    "    'OldSalesVolume': 'old_sales_volume',\n",
    "    'Detached_SemiDetached_Ratio': 'detached_semi_detached_ratio',\n",
    "    'Detached_Terraced_Ratio': 'detached_terraced_ratio',\n",
    "    'Detached_Flat_Ratio': 'detached_flat_ratio',\n",
    "    'AveragePrice_PctChange': 'average_price_pct_change',\n",
    "    'DetachedPrice_PctChange': 'detached_price_pct_change',\n",
    "    'SemiDetachedPrice_PctChange': 'semi_detached_price_pct_change',\n",
    "    'TerracedPrice_PctChange': 'terraced_price_pct_change',\n",
    "    'FlatPrice_PctChange': 'flat_price_pct_change',\n",
    "    'SalesVolume_log': 'sales_volume_log',\n",
    "    'DetachedPrice_log': 'detached_price_log',\n",
    "    'SemiDetachedPrice_log': 'semi_detached_price_log',\n",
    "    'TerracedPrice_log': 'terraced_price_log',\n",
    "    'FlatPrice_log': 'flat_price_log',\n",
    "    'AveragePrice_log': 'average_price_log',\n",
    "    'Predicted_Price_Unscaled': 'predicted_price_unscaled'\n",
    "})\n",
    "\n",
    "# Additionally derived features for analysis and additional insight\n",
    "\n",
    "ETL_stage_output['deprivation_adjusted_gdhi'] = ETL_stage_output['gdhi'] / ETL_stage_output['deprivation_average_score']\n",
    "ETL_stage_output['gdhi_per_capita'] = ETL_stage_output['gdhi'] / (ETL_stage_output['age_0_20'] + ETL_stage_output['age_20_40'] + ETL_stage_output['age_40_60'] + ETL_stage_output['age_60_plus'])\n",
    "ETL_stage_output['deprivation_employment_ratio'] = (ETL_stage_output['num_aged_16_plus_employed'] + ETL_stage_output['num_aged_16_plus_self_employed']) / ETL_stage_output['deprivation_average_score']\n",
    "ETL_stage_output['qualification_adjusted_employment_rate'] = ETL_stage_output['num_aged_16_plus_employed'] / ETL_stage_output['qualification_index_score']\n",
    "ETL_stage_output['housing_demand_indicator'] = (ETL_stage_output['rental_price'] + ETL_stage_output['sales_volume']) / ETL_stage_output['area_sq_km']\n",
    "ETL_stage_output['age_dependency_ratio'] = (ETL_stage_output['age_0_20'] + ETL_stage_output['age_60_plus']) / (ETL_stage_output['age_20_40'] + ETL_stage_output['age_40_60'])\n",
    "ETL_stage_output['deprivation_reduction_potential'] = ETL_stage_output['qualification_index_score'] / ETL_stage_output['deprivation_average_score']\n",
    "    \n",
    "info_logger.info(\"Column renaming and column addition completed \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5793a02a-cb2f-4827-bed0-5d46287a4a85",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "390b301f-80fb-4bcf-8f36-3160a9d3579b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:32.010182Z",
     "iopub.status.busy": "2024-09-12T12:39:32.009324Z",
     "iopub.status.idle": "2024-09-12T12:39:32.056574Z",
     "shell.execute_reply": "2024-09-12T12:39:32.055514Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Database credentials\n",
    "DB_CONFIG = {\n",
    "    'dbname': 'UK Real Estate DB',\n",
    "    'user': 'postgres',\n",
    "    'password': '123!@*qweQWE',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "@contextmanager\n",
    "def get_db_connection():\n",
    "    \"\"\"Context manager for PostgreSQL database connection.\"\"\"\n",
    "    connection = psycopg2.connect(**DB_CONFIG)\n",
    "    try:\n",
    "        yield connection\n",
    "    finally:\n",
    "        connection.close()\n",
    "\n",
    "def create_temp_table(cursor, table_name, schema):\n",
    "    \"\"\"Creates a temporary table.\"\"\"\n",
    "    create_query = f\"CREATE TEMPORARY TABLE {table_name} ({schema});\"\n",
    "    cursor.execute(create_query)\n",
    "\n",
    "def insert_data(cursor, table_name, columns, data_frame):\n",
    "    \"\"\"Inserts data into the temporary table.\"\"\"\n",
    "    columns_str = ', '.join(columns)\n",
    "    values_str = ', '.join(['%s'] * len(columns))\n",
    "    insert_query = f\"INSERT INTO {table_name} ({columns_str}) VALUES ({values_str});\"\n",
    "    for _, row in data_frame[columns].drop_duplicates().iterrows():\n",
    "        cursor.execute(insert_query, tuple(row))\n",
    "\n",
    "def update_existing_records(cursor, update_query):\n",
    "    \"\"\"Updates existing records in the target table.\"\"\"\n",
    "    cursor.execute(update_query)\n",
    "\n",
    "def insert_new_records(cursor, insert_query):\n",
    "    \"\"\"Inserts new records into the target table.\"\"\"\n",
    "    cursor.execute(insert_query)\n",
    "\n",
    "def drop_temp_table(cursor, table_name):\n",
    "    \"\"\"Drops the temporary table.\"\"\"\n",
    "    cursor.execute(f\"DROP TABLE {table_name};\")\n",
    "\n",
    "def load_region_dimension(incremental_df):\n",
    "    \"\"\"Performs incremental load for the region_dimension table.\"\"\"\n",
    "    with get_db_connection() as connection:\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Creating a temporary table to hold new and updated region data\n",
    "        create_temp_table(cursor, 'region_dimension_temp',\n",
    "            'region_code VARCHAR(255), region_name VARCHAR(255), local_authority_code VARCHAR(255), local_authority_name VARCHAR(255)')\n",
    "        \n",
    "        # Inserting data into the temporary table\n",
    "        insert_data(cursor, 'region_dimension_temp', \n",
    "                    ['region_code', 'region_name', 'local_authority_code', 'local_authority_name'], \n",
    "                    incremental_df)\n",
    "        \n",
    "        # Updating existing records in the region_dimension table\n",
    "        update_existing_records(cursor, \"\"\"\n",
    "        UPDATE region_dimension\n",
    "        SET \n",
    "            region_code = temp.region_code,\n",
    "            region_name = temp.region_name,\n",
    "            local_authority_name = temp.local_authority_name\n",
    "        FROM region_dimension_temp temp\n",
    "        WHERE region_dimension.local_authority_code = temp.local_authority_code\n",
    "        AND (\n",
    "            region_dimension.region_code != temp.region_code OR\n",
    "            region_dimension.region_name != temp.region_name OR\n",
    "            region_dimension.local_authority_name != temp.local_authority_name\n",
    "        );\n",
    "        \"\"\")\n",
    "        \n",
    "        # Inserting new records into the region_dimension table\n",
    "        insert_new_records(cursor, \"\"\"\n",
    "        INSERT INTO region_dimension (region_code, region_name, local_authority_code, local_authority_name)\n",
    "        SELECT temp.region_code, temp.region_name, temp.local_authority_code, temp.local_authority_name\n",
    "        FROM region_dimension_temp temp\n",
    "        LEFT JOIN region_dimension rd ON rd.local_authority_code = temp.local_authority_code\n",
    "        WHERE rd.local_authority_code IS NULL;\n",
    "        \"\"\")\n",
    "        \n",
    "        # Dropping the temporary table\n",
    "        drop_temp_table(cursor, 'region_dimension_temp')\n",
    "        connection.commit()\n",
    "        print(\"Incremental load for region_dimension completed.\")  \n",
    "        info_logger.info(\"Testing completed for Region dimension\")\n",
    "\n",
    "\n",
    "def load_date_dimension(incremental_df):\n",
    "    \"\"\"Performs incremental load for the date_dimension table.\"\"\"\n",
    "    with get_db_connection() as connection:\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Creating a temporary table\n",
    "        create_temp_table(cursor, 'date_dimension_temp',\n",
    "            'date DATE, month INT, quarter INT, year INT, transfer_month_year VARCHAR(255)')\n",
    "        \n",
    "        # Inserting data into the temporary table\n",
    "        insert_data(cursor, 'date_dimension_temp', \n",
    "                    ['date', 'month', 'quarter', 'year', 'transfer_month_year'], \n",
    "                    incremental_df)\n",
    "        \n",
    "        # Inserting new records into the date_dimension table\n",
    "        insert_new_records(cursor, \"\"\"\n",
    "        INSERT INTO date_dimension (date, month, quarter, year, transfer_month_year)\n",
    "        SELECT temp.date, temp.month, temp.quarter, temp.year, temp.transfer_month_year\n",
    "        FROM date_dimension_temp temp\n",
    "        LEFT JOIN date_dimension dd ON dd.date = temp.date\n",
    "        WHERE dd.date IS NULL;\n",
    "        \"\"\")\n",
    "        \n",
    "        # Dropping the temporary table\n",
    "        drop_temp_table(cursor, 'date_dimension_temp')\n",
    "        connection.commit()\n",
    "        print(\"Incremental load for date_dimension completed.\")\n",
    "        info_logger.info(\"Testing completed for Date dimension\")\n",
    "\n",
    "\n",
    "def load_vehicle_dimension(incremental_df):\n",
    "    \"\"\"Performs incremental load for the vehicle_dimension table.\"\"\"\n",
    "    with get_db_connection() as connection:\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Creating a temporary table to hold new and updated vehicle data\n",
    "        create_temp_table(cursor, 'vehicle_dimension_temp',\n",
    "            'local_authority_code VARCHAR(255), buses_total FLOAT, petrol_cars_total FLOAT, hgv_total FLOAT, petrol_lgv_total FLOAT, lpg_lgv_total FLOAT, hgv_motorways FLOAT, personal_transport FLOAT')\n",
    "        \n",
    "        # Inserting data into the temporary table\n",
    "        insert_data(cursor, 'vehicle_dimension_temp', \n",
    "                    ['local_authority_code', 'buses_total', 'petrol_cars_total', 'hgv_total', 'petrol_lgv_total', 'lpg_lgv_total', 'hgv_motorways', 'personal_transport'], \n",
    "                    incremental_df)\n",
    "        \n",
    "        # Updating existing records in the vehicle_dimension table\n",
    "        update_existing_records(cursor, \"\"\"\n",
    "        UPDATE vehicle_dimension\n",
    "        SET \n",
    "            region_id = rd.region_id,\n",
    "            buses_total = temp.buses_total,\n",
    "            petrol_cars_total = temp.petrol_cars_total,\n",
    "            hgv_total = temp.hgv_total,\n",
    "            petrol_lgv_total = temp.petrol_lgv_total,\n",
    "            lpg_lgv_total = temp.lpg_lgv_total,             \n",
    "            hgv_motorways = temp.hgv_motorways,            \n",
    "            personal_transport = temp.personal_transport    \n",
    "        FROM vehicle_dimension_temp temp\n",
    "        JOIN region_dimension rd ON temp.local_authority_code = rd.local_authority_code\n",
    "        WHERE vehicle_dimension.local_authority_code = temp.local_authority_code\n",
    "        AND (\n",
    "            vehicle_dimension.buses_total != temp.buses_total OR\n",
    "            vehicle_dimension.petrol_cars_total != temp.petrol_cars_total OR\n",
    "            vehicle_dimension.hgv_total != temp.hgv_total OR\n",
    "            vehicle_dimension.petrol_lgv_total != temp.petrol_lgv_total OR\n",
    "            vehicle_dimension.lpg_lgv_total != temp.lpg_lgv_total OR            \n",
    "            vehicle_dimension.hgv_motorways != temp.hgv_motorways OR            \n",
    "            vehicle_dimension.personal_transport != temp.personal_transport );\n",
    "        \"\"\")\n",
    "        \n",
    "        # Inserting new records into the vehicle_dimension table for new vehicles not previously recorded\n",
    "        insert_new_records(cursor, \"\"\"\n",
    "        INSERT INTO vehicle_dimension (local_authority_code, region_id, buses_total, petrol_cars_total, hgv_total, petrol_lgv_total, lpg_lgv_total, hgv_motorways, personal_transport)\n",
    "        SELECT temp.local_authority_code, rd.region_id, temp.buses_total, temp.petrol_cars_total, temp.hgv_total, temp.petrol_lgv_total, temp.lpg_lgv_total, temp.hgv_motorways, temp.personal_transport\n",
    "        FROM vehicle_dimension_temp temp\n",
    "        JOIN region_dimension rd ON temp.local_authority_code = rd.local_authority_code\n",
    "        LEFT JOIN vehicle_dimension vd ON vd.local_authority_code = temp.local_authority_code\n",
    "        WHERE vd.local_authority_code IS NULL;\n",
    "        \"\"\")\n",
    "        \n",
    "        # Dropping the temporary table\n",
    "        drop_temp_table(cursor, 'vehicle_dimension_temp')\n",
    "        connection.commit()\n",
    "        print(\"Incremental load for vehicle_dimension completed.\")\n",
    "        info_logger.info(\"Testing completed for Vechile dimension\")\n",
    "\n",
    "\n",
    "def load_rental_dimension(incremental_df):\n",
    "    \"\"\"Performs incremental load for the rental_dimension table.\"\"\"\n",
    "    with get_db_connection() as connection:\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Creating a temporary table to hold new and updated rental data\n",
    "        create_temp_table(cursor, 'rental_dimension_temp',\n",
    "            'local_authority_code VARCHAR(255), date DATE, rental_price FLOAT, one_bedroom_rent FLOAT, two_bedrooms_rent FLOAT, three_bedrooms_rent FLOAT, four_or_more_bedrooms_rent FLOAT, all_categories_rent FLOAT')\n",
    "        \n",
    "        # Inserting data into the temporary table\n",
    "        insert_data(cursor, 'rental_dimension_temp', \n",
    "                    ['local_authority_code', 'date', 'rental_price', 'one_bedroom_rent', 'two_bedrooms_rent', 'three_bedrooms_rent', 'four_or_more_bedrooms_rent', 'all_categories_rent'], \n",
    "                    incremental_df)\n",
    "        \n",
    "        # Updating existing records to set end_date and is_current status\n",
    "        update_existing_records(cursor, \"\"\"\n",
    "        UPDATE rental_dimension rd\n",
    "        SET end_date = NOW(), is_current = FALSE\n",
    "        FROM rental_dimension_temp temp\n",
    "        WHERE rd.local_authority_code = temp.local_authority_code\n",
    "        AND rd.is_current = TRUE\n",
    "        AND (\n",
    "            rd.date != temp.date OR\n",
    "            rd.rental_price != temp.rental_price OR\n",
    "            rd.one_bedroom_rent != temp.one_bedroom_rent OR\n",
    "            rd.two_bedrooms_rent != temp.two_bedrooms_rent OR\n",
    "            rd.three_bedrooms_rent !=temp.three_bedrooms_rent OR\n",
    "            rd.four_or_more_bedrooms_rent != temp.four_or_more_bedrooms_rent OR\n",
    "            rd.all_categories_rent != temp.all_categories_rent\n",
    "        );\n",
    "        \"\"\")\n",
    "        \n",
    "        # Inserting new or updated records into the rental_dimension table\n",
    "        insert_new_records(cursor, \"\"\"\n",
    "        INSERT INTO rental_dimension (local_authority_code, date, rental_price, one_bedroom_rent, two_bedrooms_rent, three_bedrooms_rent, four_or_more_bedrooms_rent, all_categories_rent, start_date, end_date, is_current)\n",
    "        SELECT temp.local_authority_code, temp.date, temp.rental_price, temp.one_bedroom_rent, temp.two_bedrooms_rent, temp.three_bedrooms_rent, temp.four_or_more_bedrooms_rent, temp.all_categories_rent, NOW() AS start_date, NULL AS end_date, TRUE AS is_current\n",
    "        FROM (\n",
    "            SELECT temp.*, \n",
    "                   ROW_NUMBER() OVER (PARTITION BY temp.local_authority_code ORDER BY temp.date DESC) AS rn\n",
    "            FROM rental_dimension_temp temp\n",
    "        ) temp\n",
    "        LEFT JOIN rental_dimension rd ON rd.local_authority_code = temp.local_authority_code\n",
    "        AND rd.is_current = TRUE\n",
    "        WHERE (rd.local_authority_code IS NULL OR (\n",
    "                rd.rental_price != temp.rental_price OR\n",
    "                rd.one_bedroom_rent != temp.one_bedroom_rent OR\n",
    "                rd.two_bedrooms_rent != temp.two_bedrooms_rent OR\n",
    "                rd.three_bedrooms_rent != temp.three_bedrooms_rent OR\n",
    "                rd.four_or_more_bedrooms_rent != temp.four_or_more_bedrooms_rent OR\n",
    "                rd.all_categories_rent != temp.all_categories_rent\n",
    "            ))\n",
    "        AND temp.rn = 1;\n",
    "        \"\"\")\n",
    "        \n",
    "        # Dropping the temporary table\n",
    "        drop_temp_table(cursor, 'rental_dimension_temp')\n",
    "        connection.commit()\n",
    "        print(\"Incremental load for rental_dimension completed.\")\n",
    "        info_logger.info(\"Testing completed for Rental dimension\")\n",
    "\n",
    "\n",
    "def load_district_dimension(incremental_df):\n",
    "    \"\"\"Performs incremental load for the district_dimension table.\"\"\"\n",
    "    with get_db_connection() as connection:\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Creating a temporary table to hold new and updated district data\n",
    "        create_temp_table(cursor, 'district_dimension_temp',\n",
    "            'local_authority_code VARCHAR(255), date DATE, district VARCHAR(255), town_city VARCHAR(255), county VARCHAR(255)')\n",
    "        \n",
    "        # Inserting data into the temporary table\n",
    "        insert_data(cursor, 'district_dimension_temp', \n",
    "                    ['local_authority_code', 'date', 'district', 'town_city', 'county'], \n",
    "                    incremental_df)\n",
    "        \n",
    "        # Updating existing records to set end_date and is_current status\n",
    "        update_existing_records(cursor, \"\"\"\n",
    "        UPDATE district_dimension dd\n",
    "        SET end_date = NOW(), is_current = FALSE\n",
    "        FROM district_dimension_temp temp\n",
    "        WHERE dd.local_authority_code = temp.local_authority_code\n",
    "        AND dd.is_current = TRUE\n",
    "        AND (\n",
    "            dd.date != temp.date OR\n",
    "            dd.district != temp.district OR\n",
    "            dd.town_city != temp.town_city OR\n",
    "            dd.county != temp.county\n",
    "        );\n",
    "        \"\"\")\n",
    "        \n",
    "        # Inserting new or updated records into the district_dimension table\n",
    "        insert_new_records(cursor, \"\"\"\n",
    "        INSERT INTO district_dimension (local_authority_code, date, district, town_city, county, start_date, end_date, is_current)\n",
    "        SELECT temp.local_authority_code, temp.date, temp.district, temp.town_city, temp.county, NOW() AS start_date, NULL AS end_date, TRUE AS is_current\n",
    "        FROM (\n",
    "            SELECT temp.*, \n",
    "                   ROW_NUMBER() OVER (PARTITION BY temp.local_authority_code ORDER BY temp.date DESC) AS rn\n",
    "            FROM district_dimension_temp temp\n",
    "        ) temp\n",
    "        LEFT JOIN district_dimension dd ON dd.local_authority_code = temp.local_authority_code\n",
    "        AND dd.is_current = TRUE\n",
    "        WHERE (dd.local_authority_code IS NULL OR (\n",
    "                dd.district != temp.district OR\n",
    "                dd.town_city != temp.town_city OR\n",
    "                dd.county != temp.county\n",
    "            ))\n",
    "        AND temp.rn = 1;\n",
    "        \"\"\")\n",
    "        \n",
    "        # Dropping the temporary table\n",
    "        drop_temp_table(cursor, 'district_dimension_temp')\n",
    "        connection.commit()\n",
    "        print(\"Incremental load for district_dimension completed.\")\n",
    "        info_logger.info(\"Testing completed for District dimension\")\n",
    "\n",
    "\n",
    "def load_property_type_dimension(incremental_df):\n",
    "    \"\"\"Performs incremental load for the property_type_dimension table.\"\"\"\n",
    "    with get_db_connection() as connection:\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Creating a temporary table to hold new and updated property type data\n",
    "        create_temp_table(cursor, 'property_type_dimension_temp',\n",
    "            'local_authority_code VARCHAR(255), date DATE, property_type VARCHAR(255), duration VARCHAR(255), detached_price FLOAT, semi_detached_price FLOAT, terraced_price FLOAT, flat_price FLOAT')\n",
    "        \n",
    "        # Inserting data into the temporary table\n",
    "        insert_data(cursor, 'property_type_dimension_temp', \n",
    "                    ['local_authority_code', 'date', 'property_type', 'duration', 'detached_price', 'semi_detached_price', 'terraced_price', 'flat_price'], \n",
    "                    incremental_df)\n",
    "        \n",
    "        # Updating existing records to set end_date and is_current status\n",
    "        update_existing_records(cursor, \"\"\"\n",
    "        UPDATE property_type_dimension pd\n",
    "        SET end_date = NOW(), is_current = FALSE\n",
    "        FROM property_type_dimension_temp temp\n",
    "        WHERE pd.local_authority_code = temp.local_authority_code\n",
    "        AND pd.is_current = TRUE\n",
    "        AND (\n",
    "            pd.property_type = temp.property_type OR\n",
    "            pd.duration = temp.duration OR\n",
    "            pd.date != temp.date OR\n",
    "            pd.detached_price != temp.detached_price OR\n",
    "            pd.semi_detached_price != temp.semi_detached_price OR\n",
    "            pd.terraced_price != temp.terraced_price OR\n",
    "            pd.flat_price != temp.flat_price\n",
    "        );\n",
    "        \"\"\")\n",
    "        \n",
    "        # Inserting new or updated records into the property_type_dimension table\n",
    "        insert_new_records(cursor, \"\"\"\n",
    "        INSERT INTO property_type_dimension (local_authority_code, date, property_type, duration, detached_price, semi_detached_price, terraced_price, flat_price, start_date, end_date, is_current)\n",
    "        SELECT temp.local_authority_code, temp.date, temp.property_type, temp.duration, temp.detached_price, temp.semi_detached_price, temp.terraced_price, temp.flat_price, NOW() AS start_date, NULL AS end_date, TRUE AS is_current\n",
    "        FROM (\n",
    "            SELECT temp.*, \n",
    "                   ROW_NUMBER() OVER (PARTITION BY temp.local_authority_code, temp.property_type, temp.duration ORDER BY temp.date DESC) AS rn\n",
    "            FROM property_type_dimension_temp temp\n",
    "        ) temp\n",
    "        LEFT JOIN property_type_dimension pd ON pd.local_authority_code = temp.local_authority_code\n",
    "        AND pd.is_current = TRUE\n",
    "        WHERE (pd.local_authority_code IS NULL OR (\n",
    "                pd.property_type = temp.property_type OR\n",
    "                pd.duration = temp.duration OR\n",
    "                pd.detached_price != temp.detached_price OR\n",
    "                pd.semi_detached_price != temp.semi_detached_price OR\n",
    "                pd.terraced_price != temp.terraced_price OR\n",
    "                pd.flat_price != temp.flat_price\n",
    "            ))\n",
    "        AND temp.rn = 1;\n",
    "        \"\"\")\n",
    "        \n",
    "        # Dropping the temporary table\n",
    "        drop_temp_table(cursor, 'property_type_dimension_temp')\n",
    "        connection.commit()\n",
    "        print(\"Incremental load for property_type_dimension completed.\")\n",
    "        info_logger.info(\"Testing completed for Property type dimension\")\n",
    "\n",
    "\n",
    "def load_education_employment_dimension(incremental_df):\n",
    "    \"\"\"Performs incremental load for the education_employment_dimension table.\"\"\"\n",
    "    with get_db_connection() as connection:\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Creating a temporary table to hold new and updated education and employment data\n",
    "        create_temp_table(cursor, 'education_employment_dimension_temp',\n",
    "            'local_authority_code VARCHAR(255), date DATE, qualification_index_score FLOAT, qualification_index_rank FLOAT, no_qualifications FLOAT, level_1_and_entry_level_qualifications FLOAT, level_2_qualifications FLOAT, level_3_qualifications FLOAT, apprenticeship FLOAT, level_4_qualifications_and_above FLOAT, other_qualifications FLOAT, num_aged_16_plus_unemployed FLOAT, num_aged_16_plus_employed FLOAT, num_aged_16_plus_self_employed FLOAT, deprivation_average_score FLOAT, deprivation_employment_ratio FLOAT, qualification_adjusted_employment_rate FLOAT')\n",
    "        \n",
    "        # Inserting data into the temporary table\n",
    "        insert_data(cursor, 'education_employment_dimension_temp', \n",
    "                    ['local_authority_code', 'date', 'qualification_index_score', 'qualification_index_rank', 'no_qualifications', 'level_1_and_entry_level_qualifications', 'level_2_qualifications', 'level_3_qualifications', 'apprenticeship', 'level_4_qualifications_and_above', 'other_qualifications', 'num_aged_16_plus_unemployed', 'num_aged_16_plus_employed', 'num_aged_16_plus_self_employed', 'deprivation_average_score', 'deprivation_employment_ratio', 'qualification_adjusted_employment_rate'], \n",
    "                    incremental_df)\n",
    "        \n",
    "        # Updating existing records to set end_date and is_current status\n",
    "        update_existing_records(cursor, \"\"\"\n",
    "        UPDATE education_employment_dimension ed\n",
    "        SET end_date = NOW(), is_current = FALSE\n",
    "        FROM education_employment_dimension_temp temp\n",
    "        WHERE ed.local_authority_code = temp.local_authority_code\n",
    "        AND ed.is_current = TRUE\n",
    "        AND (\n",
    "            ed.date != temp.date OR\n",
    "            ed.qualification_index_score != temp.qualification_index_score OR\n",
    "            ed.qualification_index_rank != temp.qualification_index_rank OR\n",
    "            ed.no_qualifications != temp.no_qualifications OR\n",
    "            ed.level_1_and_entry_level_qualifications != temp.level_1_and_entry_level_qualifications OR\n",
    "            ed.level_2_qualifications != temp.level_2_qualifications OR\n",
    "            ed.level_3_qualifications != temp.level_3_qualifications OR\n",
    "            ed.apprenticeship != temp.apprenticeship OR\n",
    "            ed.level_4_qualifications_and_above != temp.level_4_qualifications_and_above OR\n",
    "            ed.other_qualifications != temp.other_qualifications OR\n",
    "            ed.num_aged_16_plus_unemployed != temp.num_aged_16_plus_unemployed OR\n",
    "            ed.num_aged_16_plus_employed != temp.num_aged_16_plus_employed OR\n",
    "            ed.num_aged_16_plus_self_employed != temp.num_aged_16_plus_self_employed OR\n",
    "            ed.deprivation_average_score != temp.deprivation_average_score OR\n",
    "            ed.deprivation_employment_ratio != temp.deprivation_employment_ratio OR\n",
    "            ed.qualification_adjusted_employment_rate != temp.qualification_adjusted_employment_rate\n",
    "        );\n",
    "        \"\"\")\n",
    "        \n",
    "        # Inserting new or updated records into the education_employment_dimension table\n",
    "        insert_new_records(cursor, \"\"\"\n",
    "        INSERT INTO education_employment_dimension (local_authority_code, date, qualification_index_score, \n",
    "            qualification_index_rank, no_qualifications, level_1_and_entry_level_qualifications, level_2_qualifications, \n",
    "            level_3_qualifications, apprenticeship, level_4_qualifications_and_above, other_qualifications, \n",
    "            num_aged_16_plus_unemployed, num_aged_16_plus_employed, num_aged_16_plus_self_employed, \n",
    "            deprivation_average_score, deprivation_employment_ratio, qualification_adjusted_employment_rate, \n",
    "            start_date, end_date, is_current)\n",
    "        SELECT temp.local_authority_code, temp.date, temp.qualification_index_score, temp.qualification_index_rank, \n",
    "            temp.no_qualifications, temp.level_1_and_entry_level_qualifications, temp.level_2_qualifications, \n",
    "            temp.level_3_qualifications, temp.apprenticeship, temp.level_4_qualifications_and_above, temp.other_qualifications, \n",
    "            temp.num_aged_16_plus_unemployed, temp.num_aged_16_plus_employed, temp.num_aged_16_plus_self_employed, \n",
    "            temp.deprivation_average_score, temp.deprivation_employment_ratio, temp.qualification_adjusted_employment_rate, \n",
    "            NOW() AS start_date, NULL AS end_date, TRUE AS is_current\n",
    "        FROM (\n",
    "            SELECT temp.*, \n",
    "                   ROW_NUMBER() OVER (PARTITION BY temp.local_authority_code ORDER BY temp.date DESC) AS rn\n",
    "            FROM education_employment_dimension_temp temp\n",
    "        ) temp\n",
    "        LEFT JOIN education_employment_dimension ed ON ed.local_authority_code = temp.local_authority_code\n",
    "        AND ed.is_current = TRUE\n",
    "        WHERE (ed.local_authority_code IS NULL OR (\n",
    "                ed.qualification_index_score != temp.qualification_index_score OR\n",
    "                ed.qualification_index_rank != temp.qualification_index_rank OR\n",
    "                ed.no_qualifications != temp.no_qualifications OR\n",
    "                ed.level_1_and_entry_level_qualifications != temp.level_1_and_entry_level_qualifications OR\n",
    "                ed.level_2_qualifications != temp.level_2_qualifications OR\n",
    "                ed.level_3_qualifications != temp.level_3_qualifications OR\n",
    "                ed.apprenticeship != temp.apprenticeship OR\n",
    "                ed.level_4_qualifications_and_above != temp.level_4_qualifications_and_above OR\n",
    "                ed.other_qualifications != temp.other_qualifications OR\n",
    "                ed.num_aged_16_plus_unemployed != temp.num_aged_16_plus_unemployed OR\n",
    "                ed.num_aged_16_plus_employed != temp.num_aged_16_plus_employed OR\n",
    "                ed.num_aged_16_plus_self_employed != temp.num_aged_16_plus_self_employed OR\n",
    "                ed.deprivation_average_score != temp.deprivation_average_score OR\n",
    "                ed.deprivation_employment_ratio != temp.deprivation_employment_ratio OR\n",
    "                ed.qualification_adjusted_employment_rate != temp.qualification_adjusted_employment_rate \n",
    "            ))\n",
    "        AND temp.rn = 1;\n",
    "        \"\"\")\n",
    "        \n",
    "        # Dropping the temporary table\n",
    "        drop_temp_table(cursor, 'education_employment_dimension_temp')\n",
    "        connection.commit()\n",
    "        print(\"Incremental load for education_employment_dimension completed.\")\n",
    "        info_logger.info(\"Testing completed for Education employment dimension\")\n",
    "\n",
    "\n",
    "def load_demographics_dimension(incremental_df):\n",
    "    \"\"\"Performs incremental load for the demographics_dimension table.\"\"\"\n",
    "    with get_db_connection() as connection:\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Creating a temporary table to hold new and updated demographic data\n",
    "        create_temp_table(cursor, 'demographics_dimension_temp',\n",
    "            'local_authority_code VARCHAR(255), date DATE, area_sq_km FLOAT, age_0_20 FLOAT, age_20_40 FLOAT, age_40_60 FLOAT, age_60_plus FLOAT, female_population FLOAT, all_ages FLOAT, male_population FLOAT, age_dependency_ratio FLOAT, est_num_households_with_child FLOAT')\n",
    "        \n",
    "        # Inserting data into the temporary table\n",
    "        insert_data(cursor, 'demographics_dimension_temp', \n",
    "                    ['local_authority_code', 'date', 'area_sq_km', 'age_0_20', 'age_20_40', 'age_40_60', 'age_60_plus', 'female_population', 'all_ages', 'male_population', 'age_dependency_ratio', 'est_num_households_with_child'], \n",
    "                    incremental_df)\n",
    "        \n",
    "        # Updating existing records to set end_date and is_current status\n",
    "        update_existing_records(cursor, \"\"\"\n",
    "        UPDATE demographics_dimension dd\n",
    "        SET end_date = NOW(), is_current = FALSE\n",
    "        FROM demographics_dimension_temp temp\n",
    "        WHERE dd.local_authority_code = temp.local_authority_code\n",
    "        AND dd.is_current = TRUE\n",
    "        AND (\n",
    "            dd.date != temp.date OR\n",
    "            dd.area_sq_km != temp.area_sq_km OR\n",
    "            dd.age_0_20 != temp.age_0_20 OR\n",
    "            dd.age_20_40 != temp.age_20_40 OR\n",
    "            dd.age_40_60 != temp.age_40_60 OR\n",
    "            dd.age_60_plus != temp.age_60_plus OR\n",
    "            dd.female_population != temp.female_population OR\n",
    "            dd.all_ages != temp.all_ages OR\n",
    "            dd.male_population != temp.male_population OR\n",
    "            dd.age_dependency_ratio != temp.age_dependency_ratio OR\n",
    "            dd.est_num_households_with_child != temp.est_num_households_with_child\n",
    "        );\n",
    "        \"\"\")\n",
    "        \n",
    "        # Inserting new or updated records into the demographics_dimension table\n",
    "        insert_new_records(cursor, \"\"\"\n",
    "        INSERT INTO demographics_dimension (local_authority_code, date, area_sq_km, age_0_20, age_20_40, age_40_60, age_60_plus, female_population, all_ages, male_population, age_dependency_ratio, est_num_households_with_child, start_date, end_date, is_current)\n",
    "        SELECT temp.local_authority_code, temp.date, temp.area_sq_km, temp.age_0_20, temp.age_20_40, temp.age_40_60, temp.age_60_plus, temp.female_population, temp.all_ages, temp.male_population, temp.age_dependency_ratio, temp.est_num_households_with_child, NOW() AS start_date, NULL AS end_date, TRUE AS is_current\n",
    "        FROM (\n",
    "            SELECT temp.*, \n",
    "                   ROW_NUMBER() OVER (PARTITION BY temp.local_authority_code ORDER BY temp.date DESC) AS rn\n",
    "            FROM demographics_dimension_temp temp\n",
    "        ) temp\n",
    "        LEFT JOIN demographics_dimension dd ON dd.local_authority_code = temp.local_authority_code\n",
    "        AND dd.is_current = TRUE\n",
    "        WHERE (dd.local_authority_code IS NULL OR (\n",
    "                dd.area_sq_km != temp.area_sq_km OR\n",
    "                dd.age_0_20 != temp.age_0_20 OR\n",
    "                dd.age_20_40 != temp.age_20_40 OR\n",
    "                dd.age_40_60 != temp.age_40_60 OR\n",
    "                dd.age_60_plus != temp.age_60_plus OR\n",
    "                dd.female_population != temp.female_population OR\n",
    "                dd.all_ages != temp.all_ages OR\n",
    "                dd.male_population != temp.male_population OR\n",
    "                dd.age_dependency_ratio != temp.age_dependency_ratio OR\n",
    "                dd.est_num_households_with_child != temp.est_num_households_with_child\n",
    "            ))\n",
    "        AND temp.rn = 1;\n",
    "        \"\"\")\n",
    "        \n",
    "        # Dropping the temporary table\n",
    "        drop_temp_table(cursor, 'demographics_dimension_temp')\n",
    "        connection.commit()\n",
    "        print(\"Incremental load for demographics_dimension completed.\")\n",
    "        info_logger.info(\"Testing completed for Demographics dimension\")\n",
    "\n",
    "\n",
    "def load_sales_transaction_fact(incremental_df):\n",
    "    \"\"\"Performs incremental load for the sales_transactions_fact table.\"\"\"\n",
    "    with get_db_connection() as connection:\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Creating a temporary table to hold new and updated sales transaction data\n",
    "        create_temp_table(cursor, 'sales_transactions_fact_temp',\n",
    "            'local_authority_code VARCHAR(255), date DATE, price NUMERIC, average_price FLOAT, predicted_price_unscaled FLOAT, index FLOAT, average_price_pct_change FLOAT, annual_change_percent FLOAT, new_price FLOAT, old_price FLOAT, sales_volume FLOAT, sales_volume_log FLOAT, old_sales_volume FLOAT, detached_flat_ratio FLOAT, detached_terraced_ratio FLOAT, semi_detached_price_pct_change FLOAT, detached_semi_detached_ratio FLOAT, detached_price_log FLOAT, semi_detached_price_log FLOAT, flat_price_log FLOAT, terraced_price_pct_change FLOAT, terraced_price_log FLOAT, gdhi FLOAT, deprivation_adjusted_gdhi FLOAT, gdhi_per_capita FLOAT, foo_price FLOAT, cash_price FLOAT, mortgage_price FLOAT, housing_demand_indicator FLOAT, deprivation_reduction_potential FLOAT, flat_price_pct_change FLOAT, detached_price_pct_change FLOAT, average_price_log FLOAT, ftb_price FLOAT')\n",
    "\n",
    "        # Inserting data into the temporary table\n",
    "        insert_data(cursor, 'sales_transactions_fact_temp',\n",
    "                    ['local_authority_code', 'date', 'price', 'average_price', 'predicted_price_unscaled', 'index', 'average_price_pct_change', 'annual_change_percent', 'new_price', 'old_price', 'sales_volume', 'sales_volume_log', 'old_sales_volume', 'detached_flat_ratio', 'detached_terraced_ratio', 'semi_detached_price_pct_change', 'detached_semi_detached_ratio', 'detached_price_log', 'semi_detached_price_log', 'flat_price_log', 'terraced_price_pct_change', 'terraced_price_log', 'gdhi', 'deprivation_adjusted_gdhi', 'gdhi_per_capita', 'foo_price', 'cash_price', 'mortgage_price', 'housing_demand_indicator', 'deprivation_reduction_potential', 'flat_price_pct_change', 'detached_price_pct_change', 'average_price_log', 'ftb_price'],\n",
    "                    incremental_df)\n",
    "        \n",
    "        # Updating existing records if they have changed\n",
    "        update_existing_records(cursor, \"\"\"\n",
    "        UPDATE sales_transactions_fact AS stf\n",
    "        SET end_date = NOW(), is_current = FALSE\n",
    "        FROM sales_transactions_fact_temp AS temp\n",
    "        WHERE stf.local_authority_code = temp.local_authority_code\n",
    "        AND stf.is_current = TRUE\n",
    "        AND (\n",
    "            stf.date = temp.date OR\n",
    "            stf.price != temp.price OR\n",
    "            stf.average_price != temp.average_price OR\n",
    "            stf.predicted_price_unscaled != temp.predicted_price_unscaled OR\n",
    "            stf.index != temp.index OR\n",
    "            stf.average_price_pct_change != temp.average_price_pct_change OR\n",
    "            stf.annual_change_percent != temp.annual_change_percent OR\n",
    "            stf.new_price != temp.new_price OR\n",
    "            stf.old_price != temp.old_price OR\n",
    "            stf.sales_volume != temp.sales_volume OR\n",
    "            stf.sales_volume_log != temp.sales_volume_log OR\n",
    "            stf.old_sales_volume != temp.old_sales_volume OR\n",
    "            stf.detached_flat_ratio != temp.detached_flat_ratio OR\n",
    "            stf.detached_terraced_ratio != temp.detached_terraced_ratio OR\n",
    "            stf.semi_detached_price_pct_change != temp.semi_detached_price_pct_change OR\n",
    "            stf.detached_semi_detached_ratio != temp.detached_semi_detached_ratio OR\n",
    "            stf.detached_price_log != temp.detached_price_log OR\n",
    "            stf.semi_detached_price_log != temp.semi_detached_price_log OR\n",
    "            stf.flat_price_log != temp.flat_price_log OR\n",
    "            stf.terraced_price_pct_change != temp.terraced_price_pct_change OR\n",
    "            stf.terraced_price_log != temp.terraced_price_log OR\n",
    "            stf.gdhi != temp.gdhi OR\n",
    "            stf.deprivation_adjusted_gdhi != temp.deprivation_adjusted_gdhi OR\n",
    "            stf.gdhi_per_capita != temp.gdhi_per_capita OR\n",
    "            stf.foo_price != temp.foo_price OR\n",
    "            stf.cash_price != temp.cash_price OR\n",
    "            stf.mortgage_price != temp.mortgage_price OR\n",
    "            stf.housing_demand_indicator != temp.housing_demand_indicator OR\n",
    "            stf.deprivation_reduction_potential != temp.deprivation_reduction_potential OR\n",
    "            stf.flat_price_pct_change != temp.flat_price_pct_change OR\n",
    "            stf.detached_price_pct_change != temp.detached_price_pct_change OR\n",
    "            stf.average_price_log != temp.average_price_log OR\n",
    "            stf.ftb_price != temp.ftb_price\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        # Inserting new records with current status as active and end_date as NULL\n",
    "        insert_new_records(cursor, \"\"\"\n",
    "        INSERT INTO sales_transactions_fact (\n",
    "            local_authority_code, date, district_id, region_id, property_type_id, vehicle_id,\n",
    "            rental_id, demographics_id, education_employment_id, date_key, price, average_price,\n",
    "            predicted_price_unscaled, index, average_price_pct_change, annual_change_percent,\n",
    "            new_price, old_price, sales_volume, sales_volume_log,\n",
    "            old_sales_volume, detached_flat_ratio, detached_terraced_ratio, semi_detached_price_pct_change,\n",
    "            detached_semi_detached_ratio, detached_price_log, semi_detached_price_log,\n",
    "            flat_price_log, terraced_price_pct_change, terraced_price_log, gdhi,\n",
    "            deprivation_adjusted_gdhi, gdhi_per_capita, foo_price, cash_price, mortgage_price,\n",
    "            housing_demand_indicator, deprivation_reduction_potential, flat_price_pct_change, detached_price_pct_change, average_price_log, ftb_price, start_date, end_date, is_current\n",
    "        )\n",
    "        SELECT\n",
    "            temp.local_authority_code, temp.date, \n",
    "            dd.district_id, rd.region_id, ptd.property_type_id, vd.vehicle_id, \n",
    "            rentald.rental_id, ddemo.demographics_id, eed.education_employment_id, d.date_key,\n",
    "            temp.price, temp.average_price, temp.predicted_price_unscaled, temp.index, \n",
    "            temp.average_price_pct_change, temp.annual_change_percent, \n",
    "            temp.new_price, temp.old_price, temp.sales_volume, temp.sales_volume_log, \n",
    "            temp.old_sales_volume, temp.detached_flat_ratio, temp.detached_terraced_ratio, \n",
    "            temp.semi_detached_price_pct_change, temp.detached_semi_detached_ratio, \n",
    "            temp.detached_price_log, temp.semi_detached_price_log, temp.flat_price_log, \n",
    "            temp.terraced_price_pct_change, temp.terraced_price_log, temp.gdhi, \n",
    "            temp.deprivation_adjusted_gdhi, temp.gdhi_per_capita, temp.foo_price, temp.cash_price, \n",
    "            temp.mortgage_price, temp.housing_demand_indicator, temp.deprivation_reduction_potential,\n",
    "            temp.flat_price_pct_change, temp.detached_price_pct_change, temp.average_price_log, temp.ftb_price,\n",
    "            NOW() AS start_date, NULL AS end_date, TRUE AS is_current\n",
    "        FROM \n",
    "            sales_transactions_fact_temp temp\n",
    "        JOIN \n",
    "            district_dimension dd ON dd.local_authority_code = temp.local_authority_code AND dd.date = temp.date\n",
    "        JOIN \n",
    "            region_dimension rd ON rd.local_authority_code = temp.local_authority_code\n",
    "        JOIN \n",
    "            property_type_dimension ptd ON ptd.local_authority_code = temp.local_authority_code AND ptd.date = temp.date\n",
    "        JOIN \n",
    "            vehicle_dimension vd ON vd.local_authority_code = temp.local_authority_code\n",
    "        JOIN \n",
    "            rental_dimension rentald ON rentald.local_authority_code = temp.local_authority_code AND rentald.date = temp.date\n",
    "        JOIN \n",
    "            demographics_dimension ddemo ON ddemo.local_authority_code = temp.local_authority_code AND ddemo.date = temp.date\n",
    "        JOIN \n",
    "            education_employment_dimension eed ON eed.local_authority_code = temp.local_authority_code AND eed.date = temp.date\n",
    "        JOIN \n",
    "            date_dimension d ON d.date = temp.date\n",
    "        LEFT JOIN \n",
    "            sales_transactions_fact stf ON stf.local_authority_code = temp.local_authority_code AND stf.date = temp.date AND stf.is_current = TRUE\n",
    "        WHERE \n",
    "            stf.sales_id IS NULL;\n",
    "        \"\"\")\n",
    "    \n",
    "        # Dropping the temporary table\n",
    "        drop_temp_table(cursor, 'sales_transactions_fact_temp')\n",
    "    \n",
    "        # Commiting changes to the database\n",
    "        connection.commit()\n",
    "        print(\"Incremental load for sales_transactions_fact completed.\")\n",
    "        info_logger.info(\"Testing completed for Sales Transaction Fact dimension\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75dcfc20-4b89-4ce9-896a-182959d8ac38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:32.060406Z",
     "iopub.status.busy": "2024-09-12T12:39:32.060406Z",
     "iopub.status.idle": "2024-09-12T12:39:32.981752Z",
     "shell.execute_reply": "2024-09-12T12:39:32.981201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental load for region_dimension completed.\n",
      "Incremental load for date_dimension completed.\n",
      "Incremental load for vehicle_dimension completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental load for rental_dimension completed.\n",
      "Incremental load for district_dimension completed.\n",
      "Incremental load for property_type_dimension completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental load for education_employment_dimension completed.\n",
      "Incremental load for demographics_dimension completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental load for sales_transactions_fact completed.\n",
      "All dimensions and facts have been incrementally loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to perform incremental data loading into all dimension and fact tables.\n",
    "    \"\"\"\n",
    "    # Loading data into the region_dimension table\n",
    "    load_region_dimension(incremental_load_df)\n",
    "\n",
    "    # Loading data into the date_dimension table\n",
    "    load_date_dimension(incremental_load_df)\n",
    "\n",
    "    # Loading data into the vehicle_dimension table\n",
    "    load_vehicle_dimension(incremental_load_df)\n",
    "\n",
    "    # Loading data into the rental_dimension table\n",
    "    load_rental_dimension(incremental_load_df)\n",
    "\n",
    "    # Loading data into the district_dimension table\n",
    "    load_district_dimension(incremental_load_df)\n",
    "\n",
    "    # Loading data into the property_type_dimension table\n",
    "    load_property_type_dimension(incremental_load_df)\n",
    "\n",
    "    # Loading data into the education_employment_dimension table\n",
    "    load_education_employment_dimension(incremental_load_df)\n",
    "\n",
    "    # Loading data into the demographics_dimension table\n",
    "    load_demographics_dimension(incremental_load_df)\n",
    "\n",
    "    # Loading data into the sales_transactions_fact table\n",
    "    load_sales_transaction_fact(incremental_load_df)\n",
    "\n",
    "    print(\"All dimensions and facts have been incrementally loaded successfully.\")\n",
    "\n",
    "# Entry point for script execution\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    incremental_load_df = ETL_stage_output.copy()\n",
    "    #Executing main function\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "830d18b2-6585-4b75-8651-3c10c678a340",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:32.986886Z",
     "iopub.status.busy": "2024-09-12T12:39:32.985888Z",
     "iopub.status.idle": "2024-09-12T12:39:33.036475Z",
     "shell.execute_reply": "2024-09-12T12:39:33.035470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for all tables updated successfully.\n"
     ]
    }
   ],
   "source": [
    "##incremental call\n",
    "\n",
    "connection = psycopg2.connect(\n",
    "    dbname='UK Real Estate DB',\n",
    "    user='postgres',\n",
    "    password='123!@*qweQWE',\n",
    "    host='localhost',\n",
    "    port='5432'\n",
    ")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Listing target tables and their SCD types\n",
    "tables_metadata = [\n",
    "    {\"table_name\": \"region_dimension\"},\n",
    "    {\"table_name\": \"date_dimension\"},\n",
    "    {\"table_name\": \"sales_transactions_fact\"},\n",
    "    {\"table_name\": \"vehicle_dimension\"},\n",
    "    {\"table_name\": \"rental_dimension\"},\n",
    "    {\"table_name\": \"demographics_dimension\"},\n",
    "    {\"table_name\": \"education_employment_dimension\"},\n",
    "    {\"table_name\": \"property_type_dimension\"},\n",
    "    {\"table_name\": \"district_dimension\"}\n",
    "]\n",
    "\n",
    "# Setting the extraction timestamp as the current time\n",
    "last_extracted_date = datetime.now()\n",
    "\n",
    "# Updating metadata records\n",
    "for table in tables_metadata:\n",
    "    update_metadata_query = \"\"\"\n",
    "    UPDATE metadata\n",
    "    SET last_extracted_date = %s,\n",
    "        last_modified_date = %s\n",
    "    WHERE table_name = %s;\n",
    "    \"\"\"\n",
    "    cursor.execute(update_metadata_query, (\n",
    "        last_extracted_date,\n",
    "        last_extracted_date,\n",
    "        table['table_name']\n",
    "    ))\n",
    "\n",
    "# Committing the transaction\n",
    "connection.commit()\n",
    "\n",
    "# Closing the connection\n",
    "cursor.close()\n",
    "connection.close()\n",
    "\n",
    "print(\"Metadata for all tables updated successfully.\")\n",
    "info_logger.info(\"Metadata for all tables updated successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01b1c435-cf1b-4c7a-904b-920fdd98ee46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T12:39:33.040493Z",
     "iopub.status.busy": "2024-09-12T12:39:33.039492Z",
     "iopub.status.idle": "2024-09-12T12:39:33.272099Z",
     "shell.execute_reply": "2024-09-12T12:39:33.272099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "db_user = 'postgres'\n",
    "db_password = '123%21%40*qweQWE'\n",
    "db_host = 'localhost' \n",
    "db_port = '5432'  \n",
    "db_name = 'UK Real Estate DB'\n",
    "\n",
    "# Creating the database connection\n",
    "engine = create_engine(f'postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "\n",
    "# Loading ETL_stage_output DataFrame into a staging table in the database\n",
    "incremental_load_df.to_sql('incremental_load_etl_source_data', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62561bff-ecc7-43a5-ae32-0a9ef8527825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
